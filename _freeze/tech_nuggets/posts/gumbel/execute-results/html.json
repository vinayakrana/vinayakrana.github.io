{
  "hash": "c442c92d8e5dc654e177b7b5568e228a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Gumbel Softmax trick\"\ndescription: \"A trick to enable gradient-based optimization over discrete choices.\"\ndate: 2025-07-04\ncategories: [deep learning, differentiable programming, reparameterization]\nformat: html\nexecute: \n  enabled: True\n---\n\n#### Can Neural Networks Learn to Make Hard Choices and Still Be Trained with Backprop?\n\nImagine your model needs to choose one among several categories say, Red, Green, or Blue.\nThe obvious choice? Use argmax or sample from a categorical distribution.\nBut here's the catch: backpropagation doesn't work through discrete decisions.\n\n#### So, are we stuck?\n**Not quite**. Say hello to the **Gumbel-Softmax** trick.<br>\nYou can‚Äôt backprop through discrete sampling directly but you can use a continuous relaxation like the Gumbel-Softmax, which approximates categorical sampling in a differentiable way.\n\n#### ‚ùå Why argmax Kills Your Gradients\n\nBackpropagation needs continuous functions. But argmax is not one it‚Äôs a discrete, non-differentiable operation.\nThat means if your model makes a hard choice using argmax, the gradient signal hits a wall.\n\nLet‚Äôs see that in code:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\nlogits = torch.tensor([[2.0, 1.0, 0.1]], requires_grad=True)\nprobs = F.softmax(logits, dim=-1)\nsample = torch.argmax(probs, dim=-1)\n\nloss = sample.float().sum()\nloss.backward()  #  Throws an error\n```\n\nThe call to .backward() fails because gradients can‚Äôt flow through argmax it‚Äôs not differentiable with respect to logits.\nThe model can‚Äôt learn from this signal.\n\n### üßô‚Äç‚ôÇÔ∏è The Magic Behind Gumbel-Softmax\n\nWhat if, instead of making a hard discrete decision, your model makes a soft approximation one that‚Äôs almost one-hot but still differentiable?\n\nThat‚Äôs exactly what the Gumbel-Softmax trick does.\nIt turns the categorical sampling problem into a smooth, trainable operation.\n\nHere‚Äôs the idea:\n\n::: {#6533520d .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torch.nn.functional as F\n\ndef sample_gumbel(shape, eps=1e-20):\n    U = torch.rand(shape)\n    return -torch.log(-torch.log(U + eps) + eps)\n\ndef gumbel_softmax(logits, tau=1.0):\n    g = sample_gumbel(logits.shape)\n    return F.softmax((logits + g) / tau, dim=-1)\n\n```\n:::\n\n\n* logits are your raw scores (before softmax).\n\n* g is Gumbel noise, which injects randomness to simulate sampling.\n\n* tau is the temperature a hyperparameter that controls how sharp or soft the final distribution is.\n\n* The final output is a differentiable approximation of a one-hot vector.\n\nAs `œÑ ‚Üí 0`, the softmax output becomes almost one-hot, mimicking a discrete choice.\nAs `œÑ ‚Üí ‚àû`, it becomes uniform, meaning the model explores all choices equally.\n\n**Visual Example:** Learn to Blend Colors\nLet‚Äôs bring this to life with a colorful example.\n\nImagine your model needs to select one color from a palette Red, Green, or Blue to match a target color like purple üíú.\nBut instead of choosing just one, it learns to blend them softly, using Gumbel-Softmax!\n\n::: {#6d256703 .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport random\n\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\npalette = torch.tensor([\n    [1.0, 0.0, 0.0],  # Red\n    [0.0, 1.0, 0.0],  # Green\n    [0.0, 0.0, 1.0]   # Blue\n])\n\nlogits = torch.nn.Parameter(torch.randn(1, 3))\n\n# Target color: Purple (blend of Red + Blue)\ntarget = torch.tensor([[0.5, 0.0, 0.5]])\n\nopt = torch.optim.Adam([logits], lr=0.1)\n\nlosses = []\nfor step in range(100):\n    opt.zero_grad()\n    probs = gumbel_softmax(logits, tau=0.5)  # Sample a soft blend\n    color = probs @ palette                  # Weighted sum of RGB\n    loss = F.mse_loss(color, target)\n    losses.append(loss.item())\n    loss.backward()\n    opt.step()\n\n    if step % 10 == 0:\n        print(f\"Step {step}: Loss = {loss.item():.4f}\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStep 0: Loss = 0.0068\nStep 10: Loss = 0.0021\nStep 20: Loss = 0.1645\nStep 30: Loss = 0.1280\nStep 40: Loss = 0.0734\nStep 50: Loss = 0.1265\nStep 60: Loss = 0.1180\nStep 70: Loss = 0.1632\nStep 80: Loss = 0.0632\nStep 90: Loss = 0.1468\n```\n:::\n:::\n\n\n::: {#6c7501a7 .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nwith torch.no_grad():\n    probs = F.softmax(logits, dim=-1)  # Final softmax\n    blended = probs @ palette          # Final color\n\n    plt.imshow(blended.view(1, 1, 3).numpy())\n    plt.title(f\"Final Color ‚Äî probs={probs.numpy().round(2)}\")\n    plt.axis('off')\n    plt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](gumbel_files/figure-html/cell-4-output-1.png){width=389 height=409}\n:::\n:::\n\n\n#### *Even though we‚Äôre choosing among discrete options (RGB), Gumbel-Softmax lets the model learn differentiably by softly blending probabilities.*\n\nSome reading material: [Arxiv Link](https://arxiv.org/pdf/1611.01144)\n\n",
    "supporting": [
      "gumbel_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}