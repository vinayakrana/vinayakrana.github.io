{
  "hash": "34249ce21cdeadef2aa834e02d55917a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Gumbel Softmax trick\"\ndescription: \"A trick to enable gradient-based optimization over discrete choices.\"\ndate: 2025-07-04\ncategories: [deep learning, differentiable programming, reparameterization]\nformat: html\nexecute: \n  enabled: True\n---\n\n#### Can You Backprop Through a Discrete Choice?\nSuppose your model must choose one of several categories say, Red, Green, or Blue.\n\nYou use `argmax` or sample from a categorical distribution.\nBut then how do you backpropagate?\n\n#### Answer: You can‚Äôt.\nBut you can cheat slightly and still train using the Gumbel-Softmax trick.\n\n#### The Problem: argmax Breaks Gradients\n\n```python\nimport torch\nimport torch.nn.functional as F\n\nlogits = torch.tensor([[2.0, 1.0, 0.1]], requires_grad=True)\nprobs = F.softmax(logits, dim=-1)\nsample = torch.argmax(probs, dim=-1)\n\nloss = sample.float().sum()\nloss.backward()  #  Throws an error\n```\n\n### üßô‚Äç‚ôÇÔ∏è The Magic Behind Gumbel-Softmax\n\nOkay, you want your model to pick one option out of many. But you still want to train it with backprop?\n\nMost people try this:\n\n```python\nchoice = torch.argmax(logits, dim=-1)  # ‚ùå Not differentiable\n```\n\n##### Boom. Backprop dies.\nGradients have no idea how to flow through that hard decision.\n\n*So what if instead of a hard choice we pretend to choose using a soft, trainable mask?*\n\nThat‚Äôs exactly what the Gumbel-Softmax trick does.\n\n::: {#6533520d .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torch.nn.functional as F\n\ndef sample_gumbel(shape, eps=1e-20):\n    U = torch.rand(shape)\n    return -torch.log(-torch.log(U + eps) + eps)\n\ndef gumbel_softmax(logits, tau=1.0):\n    g = sample_gumbel(logits.shape)\n    return F.softmax((logits + g) / tau, dim=-1)\n\n```\n:::\n\n\n* logits are your unnormalized preferences\n\n* g is random Gumbel noise added to shake things up\n\n* tau is the temperature, controls how sharp or soft the final decision is\n\n* Softmax turns the noisy preferences into a smooth probability vector\n\nAs tau ‚Üí 0\nThe softmax becomes sharper, and the output becomes almost one-hot like a discrete decision!\n\n**Visual Example:** Learn to Blend Colors\nLet‚Äôs say your model needs to pick one color ‚Äî Red, Green, or Blue ‚Äî to match a target color (like purple üíú).\n\nBut instead of making a hard choice, it learns to blend the colors using Gumbel-Softmax.\n\n::: {#6d256703 .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\n# Our palette\npalette = torch.tensor([\n    [1.0, 0.0, 0.0],  # Red\n    [0.0, 1.0, 0.0],  # Green\n    [0.0, 0.0, 1.0]   # Blue\n])\n\n# Trainable selection logits\nlogits = torch.nn.Parameter(torch.randn(1, 3))\n\n# Our target is purple (red + blue)\ntarget = torch.tensor([[0.5, 0.0, 0.5]])\n\nopt = torch.optim.Adam([logits], lr=0.1)\n\n```\n:::\n\n\n::: {#522b8cea .cell execution_count=3}\n``` {.python .cell-code}\nlosses = []\n\nfor step in range(100):\n    opt.zero_grad()\n    probs = gumbel_softmax(logits, tau=0.5)\n    color = probs @ palette  # Weighted blend\n    loss = F.mse_loss(color, target)\n    losses.append(loss.item())\n    loss.backward()\n    opt.step()\n\n    if step % 10 == 0:\n        print(f\"Step {step}: Loss = {loss.item():.4f}\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStep 0: Loss = 0.0568\nStep 10: Loss = 0.1441\nStep 20: Loss = 0.1102\nStep 30: Loss = 0.1448\nStep 40: Loss = 0.1560\nStep 50: Loss = 0.0368\nStep 60: Loss = 0.0761\nStep 70: Loss = 0.1163\nStep 80: Loss = 0.1337\nStep 90: Loss = 0.0569\n```\n:::\n:::\n\n\n::: {#6c7501a7 .cell execution_count=4}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nwith torch.no_grad():\n    probs = F.softmax(logits, dim=-1)\n    blended = probs @ palette\n\n    plt.imshow(blended.view(1, 1, 3).numpy())\n    plt.title(f\"Final Color ‚Äî probs={probs.numpy().round(2)}\")\n    plt.axis('off')\n    plt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](gumbel_files/figure-html/cell-5-output-1.png){width=389 height=409}\n:::\n:::\n\n\n#### **Boom!** The model learns a soft mix of red and blue to match the purple target.\n\n#### *Even though it‚Äôs choosing among discrete categories, it‚Äôs doing it in a way that‚Äôs smooth and differentiable.*\n\n\n",
    "supporting": [
      "gumbel_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}