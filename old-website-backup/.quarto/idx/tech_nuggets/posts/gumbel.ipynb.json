{"title":"Gumbel Softmax trick","markdown":{"yaml":{"title":"Gumbel Softmax trick","description":"A trick to enable gradient-based optimization over discrete choices.","date":"2025-07-04","categories":["deep learning","differentiable programming","reparameterization"],"format":"html","execute":{"enabled":true}},"headingText":"Can Neural Networks Learn to Make Hard Choices and Still Be Trained with Backprop?","containsRefs":false,"markdown":"\n\n\nImagine your model needs to choose one among several categories say, Red, Green, or Blue.\nThe obvious choice? Use argmax or sample from a categorical distribution.\nBut here's the catch: backpropagation doesn't work through discrete decisions.\n\n#### So, are we stuck?\n**Not quite**. Say hello to the **Gumbel-Softmax** trick.<br>\nYou canâ€™t backprop through discrete sampling directly but you can use a continuous relaxation like the Gumbel-Softmax, which approximates categorical sampling in a differentiable way.\n\n#### Why argmax Kills Your Gradients\n\nBackpropagation needs continuous functions. But argmax is not one itâ€™s a discrete, non-differentiable operation.\nThat means if your model makes a hard choice using argmax, the gradient signal hits a wall.\n\nLetâ€™s see that in code:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\nlogits = torch.tensor([[2.0, 1.0, 0.1]], requires_grad=True)\nprobs = F.softmax(logits, dim=-1)\nsample = torch.argmax(probs, dim=-1)\n\nloss = sample.float().sum()\nloss.backward()  #  Throws an error\n```\n\nThe call to .backward() fails because gradients canâ€™t flow through argmax itâ€™s not differentiable with respect to logits.\nThe model canâ€™t learn from this signal.\n\n### The Magic Behind Gumbel-Softmax\n\nWhat if, instead of making a hard discrete decision, your model makes a soft approximation one thatâ€™s almost one-hot but still differentiable?\n\nThatâ€™s exactly what the Gumbel-Softmax trick does.\nIt turns the categorical sampling problem into a smooth, trainable operation.\n\nHereâ€™s the idea:\n\n* logits are your raw scores (before softmax).\n\n* g is Gumbel noise, which injects randomness to simulate sampling.\n\n* tau is the temperature a hyperparameter that controls how sharp or soft the final distribution is.\n\n* The final output is a differentiable approximation of a one-hot vector.\n\nAs `Ï„ â†’ 0`, the softmax output becomes almost one-hot, mimicking a discrete choice.\nAs `Ï„ â†’ âˆž`, it becomes uniform, meaning the model explores all choices equally.\n\n**Visual Example:** Learn to Blend Colors\nLetâ€™s bring this to life with a colorful example.\n\nImagine your model needs to select one color from a palette Red, Green, or Blue to match a target color like purple ðŸ’œ.\nBut instead of choosing just one, it learns to blend them softly, using Gumbel-Softmax!\n\n#### *Even though weâ€™re choosing among discrete options (RGB), Gumbel-Softmax lets the model learn differentiably by softly blending probabilities.*\n\nSome reading material: [Arxiv Link](https://arxiv.org/pdf/1611.01144)\n","srcMarkdownNoYaml":"\n\n#### Can Neural Networks Learn to Make Hard Choices and Still Be Trained with Backprop?\n\nImagine your model needs to choose one among several categories say, Red, Green, or Blue.\nThe obvious choice? Use argmax or sample from a categorical distribution.\nBut here's the catch: backpropagation doesn't work through discrete decisions.\n\n#### So, are we stuck?\n**Not quite**. Say hello to the **Gumbel-Softmax** trick.<br>\nYou canâ€™t backprop through discrete sampling directly but you can use a continuous relaxation like the Gumbel-Softmax, which approximates categorical sampling in a differentiable way.\n\n#### Why argmax Kills Your Gradients\n\nBackpropagation needs continuous functions. But argmax is not one itâ€™s a discrete, non-differentiable operation.\nThat means if your model makes a hard choice using argmax, the gradient signal hits a wall.\n\nLetâ€™s see that in code:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\nlogits = torch.tensor([[2.0, 1.0, 0.1]], requires_grad=True)\nprobs = F.softmax(logits, dim=-1)\nsample = torch.argmax(probs, dim=-1)\n\nloss = sample.float().sum()\nloss.backward()  #  Throws an error\n```\n\nThe call to .backward() fails because gradients canâ€™t flow through argmax itâ€™s not differentiable with respect to logits.\nThe model canâ€™t learn from this signal.\n\n### The Magic Behind Gumbel-Softmax\n\nWhat if, instead of making a hard discrete decision, your model makes a soft approximation one thatâ€™s almost one-hot but still differentiable?\n\nThatâ€™s exactly what the Gumbel-Softmax trick does.\nIt turns the categorical sampling problem into a smooth, trainable operation.\n\nHereâ€™s the idea:\n\n* logits are your raw scores (before softmax).\n\n* g is Gumbel noise, which injects randomness to simulate sampling.\n\n* tau is the temperature a hyperparameter that controls how sharp or soft the final distribution is.\n\n* The final output is a differentiable approximation of a one-hot vector.\n\nAs `Ï„ â†’ 0`, the softmax output becomes almost one-hot, mimicking a discrete choice.\nAs `Ï„ â†’ âˆž`, it becomes uniform, meaning the model explores all choices equally.\n\n**Visual Example:** Learn to Blend Colors\nLetâ€™s bring this to life with a colorful example.\n\nImagine your model needs to select one color from a palette Red, Green, or Blue to match a target color like purple ðŸ’œ.\nBut instead of choosing just one, it learns to blend them softly, using Gumbel-Softmax!\n\n#### *Even though weâ€™re choosing among discrete options (RGB), Gumbel-Softmax lets the model learn differentiably by softly blending probabilities.*\n\nSome reading material: [Arxiv Link](https://arxiv.org/pdf/1611.01144)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":true,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"a11y-dark","output-file":"gumbel.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.40","theme":"darkly","mainfont":"Roboto","code-copy":true,"title":"Gumbel Softmax trick","description":"A trick to enable gradient-based optimization over discrete choices.","date":"2025-07-04","categories":["deep learning","differentiable programming","reparameterization"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}